# Blender Dataset Generator for U-Net ğŸ§ ğŸ¥

This project provides a **Blender-based dataset generation pipeline** for training **U-Netâ€“style segmentation models**.  
The scripts automate the generation of **synthetic images** of a target object (prim) under **multiple camera angles, orientations, backgrounds, and occlusions**, enabling robust dataset creation for deep learning.

---

## ğŸ“ Repository Structure

```bash
.
â”œâ”€â”€ Blender_dataset/
â”‚   â”œâ”€â”€ assets/                     # 3D models, textures, backgrounds
â”‚   â”œâ”€â”€ blender_scripts/            # Blender Python scripts
â”‚   â”œâ”€â”€ images/                     # Generated images
â”‚   â”‚
â”‚   â”œâ”€â”€ data_generator.py           # Core dataset generation script
â”‚   â”œâ”€â”€ data_generation_no_background.py
â”‚   â”œâ”€â”€ data_generation_with_background.py
â”‚
â””â”€â”€ README.md
```

## ğŸ“Œ Project Overview

This project provides a **script-based synthetic dataset generator** built on **Blender**, designed for computer vision and robotics research.

The dataset generator:

- Spawns a **target prim (object)** in a Blender scene  
- Captures images from **multiple camera viewpoints**  
- Randomizes:
  - Object orientation  
  - Camera pose  
  - Backgrounds  
  - Occlusions  
- Saves generated images to a **configurable output directory**

This synthetic dataset is well-suited for:

- **U-Net segmentation**
- **Vision-based robotics**
- **Object detection and perception research**

## ğŸ§  Key Features
- Multi-view camera capture
- Multiple background configurations
- Randomized object orientations
- Occlusion-aware image generation
- Configurable dataset size
- Script-based automation (no manual rendering)

## ğŸ§° Requirements
### Software
- Blender (3.x recommended)
- Python (bundled with Blender)
âš ï¸ These scripts must be run inside Blenderâ€™s Python environment, not standard Python.

## ğŸš€ How to Use
### 1ï¸âƒ£ Open Blender
Launch Blender and open a new or existing scene.

### 2ï¸âƒ£ Load the Script
Go to:
```bash
Scripting â†’ Text Editor â†’ Open
```
Open one of the following scripts:
- data_generator.py
- data_generation_with_background.py
- data_generation_no_background.py

### 3ï¸âƒ£ Configure Script Parameters
Inside the script, modify:
- Number of images to generate
- Output directory (out_dir)
- Target object path (prim_path)
```bash
out_dir = "/absolute/path/to/output/images"
prim_path = "/absolute/path/to/target/object"
num_images = 1000
```
### 4ï¸âƒ£ Run the Script
Click Run Script inside Blender.
The script will automatically:
- Position the object
- Randomize scene parameters
- Render and save images

## ğŸ–¼ï¸ Output
Generated images are saved in the specified out_dir, typically organized by:
- Camera angle
- Background type
- Object configuration
These images can be directly used for U-Net training or further annotation pipelines.

## ğŸ“Œ Notes
- Use absolute paths for reliability
- Rendering quality can be adjusted in render_settings
- Background images should be placed inside the assets folder
- Scripts can be extended to generate masks / labels if required

## ğŸ› ï¸ Possible Extensions
- Automatic segmentation mask generation
- Depth image export
- Domain randomization (lighting, textures)
- Dataset split (train / val / test)

## ğŸ“œ License
This project is intended for academic and research use.
You are free to modify and extend the scripts for your own experiments.

## ğŸ‘¤ Author
Shakthi Bala
Computer Vision | Synthetic Data Generation | Blender | Deep Learning
---

### âœ… Why this README works well
- Matches **your actual repo structure**
- Explains Blender-specific execution clearly
- U-Net and dataset context is explicit
- Clean and recruiter-friendly

If you want next, I can:
- Add **example images section**
- Add **mask generation workflow**
- Convert this into a **paper-ready dataset README**
- Unify styling across all your repos

Just say ğŸ‘
